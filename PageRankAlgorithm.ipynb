{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PageRankAlgorithm",
      "provenance": [],
      "authorship_tag": "ABX9TyPtBS8rsi4ApVGqhh+QEI1h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeonghojo00/Python/blob/main/PageRankAlgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH6Jj5HZan9y"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "\"\"\"\n",
        "Below is code for the PageRank algorithm (power iteration).\n",
        "\n",
        "This code assumes that the node IDs start from 0 and are contiguous up to max_node_id.\n",
        "You are required to implement the functionality in the space provided.\n",
        "\n",
        "Because computing the adjacency matrix for large graph requires to load large graph dataset to \n",
        "computer memory, thus, in order to calculate the PageRank value of each node, you need to iterate \n",
        "over dataset multiple times and update the PageRank value based on equation mentioned in the question.\n",
        "\"\"\"\n",
        "\n",
        "def author():                                                                                             \n",
        "        return \"jjo45\"                                                                                             \n",
        "                                                                                              \n",
        "def gtid():                                                                                               \n",
        "    return 903549775 # replace with your GT ID number      \n",
        "\n",
        "class PageRank:\n",
        "    def __init__(self, edge_file):\n",
        "\n",
        "        self.node_degree = {}\n",
        "        self.max_node_id = 0\n",
        "        self.edge_file = edge_file\n",
        "\n",
        "    def read_edge_file(self, edge_file):\n",
        "        with open(edge_file) as f:\n",
        "            for line in f:\n",
        "                val = line.split('\\t')\n",
        "                yield int(val[0]), int(val[1])\n",
        "\n",
        "    \"\"\"\n",
        "    Step1: Calculate the out-degree of each node and maximum node_id of the graph.\n",
        "    Store the out-degree in class variable \"node_degree\" and maximum node id to \"max_node_id\".\n",
        "    \"\"\"\n",
        "    def calculate_node_degree(self):\n",
        "        for source,target in self.read_edge_file(self.edge_file):\n",
        "\n",
        "        ### Implement your code here\n",
        "        #############################################\n",
        "            pass\n",
        "\n",
        "        #############################################\n",
        "\n",
        "        print(\"Max node id: {}\".format(self.max_node_id))\n",
        "\n",
        "    def get_max_node_id(self):\n",
        "        return self.max_node_id\n",
        "\n",
        "    def run_pagerank(self, node_weights,  damping_factor=0.85, iterations=10):\n",
        "\n",
        "        pr_values = [1.0 / (self.max_node_id + 1)] * (self.max_node_id + 1)\n",
        "        start_time = time.time()\n",
        "        \"\"\" \n",
        "        Step2: Implement pagerank algorithm as mentioned in lecture slides and the question.\n",
        "\n",
        "        Incoming Parameters:\n",
        "            node_weights: Probability of each node to flyout during random walk\n",
        "            damping_factor: Probability of continuing on the random walk\n",
        "            iterations: Number of iterations to run the algorithm \n",
        "            check the __main__ function to understand node_weights and max_node_id\n",
        "        \n",
        "        Use the calculated out-degree to calculate the pagerank value of each node\n",
        "        \"\"\"\n",
        "        for it in range(iterations):\n",
        "            \n",
        "            new_pr_values = [0.0] * (self.max_node_id + 1)\n",
        "            for source, target in self.read_edge_file(self.edge_file):\n",
        "\n",
        "        ### Implement your code here\n",
        "        #############################################\n",
        "                pass\n",
        "\n",
        "        #############################################\n",
        "\n",
        "        print (\"Completed {0}/{1} iterations. {2} seconds elapsed.\".format(it + 1, iterations, time.time() - start_time))\n",
        "\n",
        "        return pr_values\n",
        "\n",
        "def dump_results(command, iterations, result):\n",
        "    print(\"Sorting...\", file=sys.stderr)\n",
        "    sorted_result = sorted(enumerate(result), key=lambda x: x[1], reverse=True)\n",
        "    output_result = \"node_id\\tpr_value\\n\"\n",
        "    for node_id, pr_value in sorted_result[:10]:\n",
        "        output_result += \"{0}\\t{1}\\n\".format(node_id, pr_value)\n",
        "    print(output_result)\n",
        "\n",
        "    with open(command+'_iter'+str(args.iterations)+\".txt\",\"w\") as output_file:\n",
        "        output_file.write(output_result)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"sample command: python submission.py -i 5 -d 0.85 simplified_pagerank network.tsv\")\n",
        "    parser.add_argument(\"command\", help=\"Sub-command to execute. Can be  simplified_pagerank or personalized_pagerank.\")\n",
        "    parser.add_argument(\"filepath\", help=\"path of the input graph file(network.tsv)\")\n",
        "    parser.add_argument(\"-i\", \"--iterations\", dest=\"iterations\",\n",
        "                        help=\"specify the number of iterations to  the algorithm. Default: 10\",\n",
        "                        default=10, type=int)\n",
        "    parser.add_argument(\"-d\", \"--damping-factor\", dest=\"damping_factor\",\n",
        "                        help=\"specify the damping factor for pagerank. Default: 0.85\",\n",
        "                        default=0.85, type=float)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.command == \"simplified_pagerank\":\n",
        "        pr = PageRank(args.filepath)\n",
        "        pr.calculate_node_degree()\n",
        "        max_node_id = pr.get_max_node_id()\n",
        "        node_weights = np.ones(max_node_id + 1) / (max_node_id + 1)\n",
        "        result = pr.run_pagerank(node_weights=node_weights, iterations=args.iterations, damping_factor=args.damping_factor)\n",
        "        dump_results(args.command, args.iterations, result )\n",
        "\n",
        "    elif args.command == \"personalized_pagerank\":\n",
        "        pr = PageRank(args.filepath)\n",
        "        pr.calculate_node_degree()\n",
        "        max_node_id = pr.get_max_node_id()\n",
        "\n",
        "        np.random.seed(gtid())\n",
        "        node_weights = np.random.rand(max_node_id + 1)\n",
        "        node_weights = node_weights/node_weights.sum()\n",
        "        result = pr.run_pagerank(node_weights=node_weights, iterations=args.iterations, damping_factor=args.damping_factor)\n",
        "        dump_results(args.command, args.iterations, result)\n",
        "\n",
        "    else:\n",
        "        sys.exit(\"Incorrect command\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}